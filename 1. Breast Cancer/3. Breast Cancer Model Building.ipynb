{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import xgboost\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "import statsmodels.api as sm\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,roc_auc_score,classification_report,precision_score,recall_score,roc_curve,auc,accuracy_score,f1_score\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import xgboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,classification_report,precision_score,recall_score,roc_curve,auc,accuracy_score,f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldata = pd.read_csv(\"Selected_Breast_cancer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the outliers using quantile methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texture_mean\n",
      "Lower Bound = 0  Upper Bound = 0.69\n",
      "min value = 0.0  max value = 1.0\n",
      "positive Outliers 7\n",
      "==================================================\n",
      "concavity_mean\n",
      "Lower Bound = 0  Upper Bound = 0.66\n",
      "min value = 0.0  max value = 1.0\n",
      "positive Outliers 18\n",
      "==================================================\n",
      "symmetry_mean\n",
      "Lower Bound = 0.03  Upper Bound = 0.71\n",
      "min value = 0.0  max value = 1.0\n",
      "negative Outliers 1\n",
      "positive Outliers 14\n",
      "==================================================\n",
      "texture_se\n",
      "Lower Bound = 0  Upper Bound = 0.46\n",
      "min value = 0.0  max value = 1.0\n",
      "positive Outliers 20\n",
      "==================================================\n",
      "area_se\n",
      "Lower Bound = 0  Upper Bound = 0.15\n",
      "min value = 0.0  max value = 1.0000000000000002\n",
      "positive Outliers 65\n",
      "==================================================\n",
      "fractal_dimension_se\n",
      "Lower Bound = 0  Upper Bound = 0.25\n",
      "min value = 0.0  max value = 1.0\n",
      "positive Outliers 28\n",
      "==================================================\n",
      "diagnosis\n",
      "Lower Bound = 0  Upper Bound = 2.5\n",
      "min value = 0  max value = 1\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in finaldata.columns:\n",
    "    Q1,Q3 = np.quantile(finaldata[i],[0.25,0.75])\n",
    "    IQR = Q3-Q1\n",
    "    lower_bound = Q1 - (1.5*IQR)\n",
    "    if lower_bound < 0:  # as in whole dataset there is no -ve value hence we set lowerbound to 0\n",
    "        lower_bound=0\n",
    "    upper_bound = Q3 + (1.5*IQR)\n",
    "    print(i)\n",
    "    print('Lower Bound =',np.round(lower_bound,2),' Upper Bound =',np.round(upper_bound,2))\n",
    "    print('min value =',finaldata[i].min(), ' max value =', finaldata[i].max())\n",
    "\n",
    "    if finaldata[i].min() < lower_bound:\n",
    "        print('negative Outliers',len(finaldata[(finaldata[i]<lower_bound)]))\n",
    "    \n",
    "    if finaldata[i].max() > upper_bound:\n",
    "        print('positive Outliers', len(finaldata[(finaldata[i]>upper_bound)]))  \n",
    "        \n",
    "   \n",
    "        \n",
    "       \n",
    "    \n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= finaldata.drop('diagnosis',axis =1)\n",
    "y= finaldata['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 6), (114, 6), (455,), (114,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.8,random_state=42)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization : GridSearch with Cross Validation for tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#from sklearn.ensemble import ExtraTreesClassifier\n",
    "# import xgboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad  = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Algo_tuning = []\n",
    "Training_tuning = []\n",
    "Testing_tuning = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "Wall time: 5.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# A parameter grid for Logistic Regression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "parameters = {'penalty':['l1', 'l2'],'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "'class_weight' :[{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}],\n",
    "'solver' : ['liblinear', 'saga'] }\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(clf, parameters,cv=10,scoring= 'accuracy', n_jobs= -1, verbose= 3)\n",
    "    \n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the knn to the best combination of parameters\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Train the model using the training sets \n",
    "clf.fit(X_train,y_train)\n",
    "y_pred_clf_test = clf.predict(X_test)\n",
    "y_pred_clf_train = clf.predict(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression model training :  0.9604395604395605\n",
      "Accuracy of Logistic Regression model testing :  0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy\n",
    "acc_clf_test =  metrics.accuracy_score(y_test, y_pred_clf_test) \n",
    "acc_clf_train =  metrics.accuracy_score(y_train, y_pred_clf_train) \n",
    "\n",
    "print( 'Accuracy of Logistic Regression model training : ', acc_clf_train )\n",
    "print( 'Accuracy of Logistic Regression model testing : ', acc_clf_test )\n",
    "\n",
    "roc_clf_train =metrics.roc_auc_score(y_train,y_pred_clf_train )\n",
    "roc_clf_test = metrics.roc_auc_score(y_test,y_pred_clf_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Algo_tuning.append('Logistic Regression')\n",
    "Training_tuning.append(acc_clf_train)\n",
    "Testing_tuning.append(acc_clf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 128 candidates, totalling 1280 fits\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a Support Vector Classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "parameters = {\n",
    "  'kernel': ['linear','rbf'],\n",
    "  'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000], \n",
    "   'class_weight' :[{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}],\n",
    "  'gamma': [0.001, 0.0001],\n",
    "    'probability':[True]\n",
    "             }\n",
    "   \n",
    "\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(svc, parameters,cv=10,scoring= 'accuracy', n_jobs= -1, verbose= 3)\n",
    "    \n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the svc to the best combination of parameters\n",
    "svc = grid_obj.best_estimator_\n",
    "\n",
    "\n",
    "# Train the model using the training sets \n",
    "svc.fit(X_train,y_train)\n",
    "y_pred_svc_test = svc.predict(X_test)\n",
    "y_pred_svc_train = svc.predict(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVC model training :  0.9538461538461539\n",
      "Accuracy of SVC model testing :  0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy\n",
    "acc_svc_test =  metrics.accuracy_score(y_test, y_pred_svc_test)\n",
    "acc_svc_train = metrics.accuracy_score(y_train, y_pred_svc_train) \n",
    "print( 'Accuracy of SVC model training : ', acc_svc_train )\n",
    "print( 'Accuracy of SVC model testing : ', acc_svc_test )\n",
    "\n",
    "roc_svc_train =metrics.roc_auc_score(y_train,y_pred_svc_train )\n",
    "roc_svc_test = metrics.roc_auc_score(y_test,y_pred_svc_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Algo_tuning.append('SVC')\n",
    "Training_tuning.append(acc_svc_train)\n",
    "Testing_tuning.append(acc_svc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 256 candidates, totalling 2560 fits\n",
      "Wall time: 7.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# K - Nearest Neighbors\n",
    "\n",
    "# Create a KNN Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "parameters = {'n_neighbors': [3, 4, 5, 10], \n",
    "              'weights': ['uniform', 'distance'],\n",
    "              'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "              'leaf_size' : [10, 20, 30, 50],\n",
    "              'p':[1,2]\n",
    "             }\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(knn, parameters,cv=10,scoring= 'accuracy', n_jobs= -1, verbose= 3)\n",
    "\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the knn to the best combination of parameters\n",
    "knn = grid_obj.best_estimator_\n",
    "\n",
    "# Train the model using the training sets \n",
    "knn.fit(X_train,y_train)\n",
    "y_pred_knn_test = knn.predict(X_test)\n",
    "y_pred_knn_train = knn.predict(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN model training :  1.0\n",
      "Accuracy of KNN model testing :  0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy\n",
    "acc_knn_test =  metrics.accuracy_score(y_test, y_pred_knn_test)\n",
    "acc_knn_train = metrics.accuracy_score(y_train, y_pred_knn_train)\n",
    "\n",
    "print( 'Accuracy of KNN model training : ', acc_knn_train )\n",
    "print( 'Accuracy of KNN model testing : ', acc_knn_test )\n",
    "\n",
    "roc_knn_train =metrics.roc_auc_score(y_train,y_pred_knn_train )\n",
    "roc_knn_test = metrics.roc_auc_score(y_test,y_pred_knn_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Algo_tuning.append('KNeighborsClassifier')\n",
    "Training_tuning.append(acc_knn_train)\n",
    "Testing_tuning.append(acc_knn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "Wall time: 2.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a GaussianNB\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "parameters = [{\n",
    " 'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "   \n",
    "]\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(nb, parameters,verbose=1, cv=10, n_jobs=-1,scoring= 'accuracy')\n",
    "\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the svc to the best combination of parameters\n",
    "nb = grid_obj.best_estimator_\n",
    "\n",
    "\n",
    "# Train the model using the training sets \n",
    "nb.fit(X_train,y_train)\n",
    "y_pred_nb_test = nb.predict(X_test)\n",
    "y_pred_nb_train = nb.predict(X_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GaussianNB model training :  0.9120879120879121\n",
      "Accuracy of GaussianNB model testing :  0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy\n",
    "acc_nb_test = metrics.accuracy_score(y_test, y_pred_nb_test) \n",
    "acc_nb_train = metrics.accuracy_score(y_train, y_pred_nb_train) \n",
    "\n",
    "print( 'Accuracy of GaussianNB model training : ', acc_nb_train )\n",
    "print( 'Accuracy of GaussianNB model testing : ', acc_nb_test )\n",
    "\n",
    "roc_nb_train =metrics.roc_auc_score(y_train,y_pred_nb_train )\n",
    "roc_nb_test = metrics.roc_auc_score(y_test,y_pred_nb_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Algo_tuning.append('GaussianNB')\n",
    "Training_tuning.append(acc_nb_train)\n",
    "Testing_tuning.append(acc_nb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 3840 candidates, totalling 38400 fits\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DTT = DecisionTreeClassifier()\n",
    "# Hyperparameter Optimization\n",
    "parameters = {'max_features': ['log2', 'sqrt','auto'], \n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [2, 3, 5, 10, 50], \n",
    "              'min_samples_split': [2, 3, 50, 100],\n",
    "              'min_samples_leaf': [1, 5, 8, 10],\n",
    "              'splitter':['best',\"random\"],\n",
    "            'random_state':[10,51,42,101]\n",
    "             }\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(DTT, parameters,cv=10,scoring= 'accuracy', n_jobs= -1, verbose=3)\n",
    "\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "DTT = grid_obj.best_estimator_\n",
    "\n",
    "# Train the model using the training sets \n",
    "DTT.fit(X_train,y_train)\n",
    "y_pred_DTT_test = DTT.predict(X_test)\n",
    "y_pred_DTT_train = DTT.predict(X_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTreeClassifier model training :  1.0\n",
      "Accuracy of DecisionTreeClassifier model testing :  0.9035087719298246\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy\n",
    "acc_DTT_test = metrics.accuracy_score(y_test, y_pred_DTT_test)\n",
    "acc_DTT_train = metrics.accuracy_score(y_train, y_pred_DTT_train) \n",
    "\n",
    "print( 'Accuracy of DecisionTreeClassifier model training : ', acc_DTT_train )\n",
    "print( 'Accuracy of DecisionTreeClassifier model testing : ', acc_DTT_test )\n",
    "\n",
    "roc_DTT_train =metrics.roc_auc_score(y_train,y_pred_DTT_train )\n",
    "roc_DTT_test = metrics.roc_auc_score(y_test,y_pred_DTT_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Algo_tuning.append('DecisionTreeClassifier')\n",
    "Training_tuning.append(acc_DTT_train)\n",
    "Testing_tuning.append(acc_DTT_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Random Forest Classifier\n",
    "\n",
    "# Import library of RandomForestClassifier model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "parameters = {'n_estimators': [4, 6, 9, 10, 15], \n",
    "              'max_features': ['log2', 'sqrt','auto'], \n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [2, 3, 5, 10], \n",
    "              'min_samples_split': [2, 3, 5],\n",
    "              'min_samples_leaf': [1, 5, 8],\n",
    "               }\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(rf, parameters)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the rf to the best combination of parameters\n",
    "rf = grid_obj.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "# Train the model using the training sets \n",
    "rf.fit(X_train,y_train)\n",
    "y_pred_rf_test = rf.predict(X_test)\n",
    "y_pred_rf_train = rf.predict(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest Classifier model training :  0.9626373626373627\n",
      "Accuracy of Random Forest Classifier model testing :  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy\n",
    "acc_rf_test =  metrics.accuracy_score(y_test, y_pred_rf_test) \n",
    "acc_rf_train =  metrics.accuracy_score(y_train, y_pred_rf_train) \n",
    "\n",
    "print( 'Accuracy of Random Forest Classifier model training : ', acc_rf_train )\n",
    "print( 'Accuracy of Random Forest Classifier model testing : ', acc_rf_test )\n",
    "\n",
    "roc_rf_train =metrics.roc_auc_score(y_train,y_pred_rf_train )\n",
    "roc_rf_test = metrics.roc_auc_score(y_test,y_pred_rf_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Algo_tuning.append('RandomForestClassifier')\n",
    "Training_tuning.append(acc_rf_train)\n",
    "Testing_tuning.append(acc_rf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 80 candidates, totalling 800 fits\n",
      "Wall time: 6.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adb = AdaBoostClassifier()\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "parameters = [{\n",
    "'n_estimators': [4, 6, 9, 10, 15], \n",
    "'learning_rate': [(0.97 + x / 100) for x in range(0, 8)],\n",
    "'algorithm': ['SAMME', 'SAMME.R']\n",
    "}]\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(adb, parameters,verbose=1, scoring = 'accuracy',cv=10, n_jobs=-1)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the svc to the best combination of parameters\n",
    "adb = grid_obj.best_estimator_\n",
    "\n",
    "\n",
    "# Train the model using the training sets \n",
    "adb.fit(X_train,y_train)\n",
    "y_pred_adb_test = adb.predict(X_test)\n",
    "y_pred_adb_train = adb.predict(X_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of adb model training :  0.9604395604395605\n",
      "Accuracy of adb model testing :  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# Calculating the accuracy\n",
    "acc_adb_test =  metrics.accuracy_score(y_test, y_pred_adb_test) \n",
    "acc_adb_train =  metrics.accuracy_score(y_train, y_pred_adb_train) \n",
    "\n",
    "print( 'Accuracy of adb model training : ', acc_adb_train )\n",
    "print( 'Accuracy of adb model testing : ', acc_adb_test )\n",
    "\n",
    "roc_adb_train =metrics.roc_auc_score(y_train,y_pred_adb_train )\n",
    "roc_adb_test = metrics.roc_auc_score(y_test,y_pred_adb_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Algo_tuning.append('AdaBoostClassifier')\n",
    "Training_tuning.append(acc_adb_train)\n",
    "Testing_tuning.append(acc_adb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 7 candidates, totalling 70 fits\n",
      "Accuracy of etc model training :  1.0\n",
      "Accuracy of etc model testing :  0.9824561403508771\n",
      "Wall time: 2.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    # Create a ExtraTreesClassifier\n",
    "    etc = ExtraTreesClassifier()\n",
    "\n",
    "    # Hyperparameter Optimization\n",
    "    parameters = [{\n",
    "    'n_estimators': [10,20,30,50,100,200,300]}]\n",
    "    # Run the grid search\n",
    "    grid_obj = GridSearchCV(etc, parameters,verbose=1, scoring = 'accuracy',cv=10, n_jobs=-1)\n",
    "    grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "    # Set the svc to the best combination of parameters\n",
    "    etc = grid_obj.best_estimator_\n",
    "\n",
    "\n",
    "    # Train the model using the training sets \n",
    "    etc.fit(X_train,y_train)\n",
    "    y_pred_etc_test = etc.predict(X_test)\n",
    "    y_pred_etc_train = etc.predict(X_train)\n",
    "    # Calculating the accuracy\n",
    "    acc_etc_test = metrics.accuracy_score(y_test, y_pred_etc_test)\n",
    "    acc_etc_train =  metrics.accuracy_score(y_train, y_pred_etc_train)\n",
    "\n",
    "    print( 'Accuracy of etc model training : ', acc_etc_train )\n",
    "    print( 'Accuracy of etc model testing : ', acc_etc_test )\n",
    "\n",
    "    roc_etc_train =metrics.roc_auc_score(y_train,y_pred_etc_train )\n",
    "    roc_etc_test = metrics.roc_auc_score(y_test,y_pred_etc_test )\n",
    "   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Algo_tuning.append('ExtraTreesClassifier')\n",
    "Training_tuning.append(acc_etc_train)\n",
    "Testing_tuning.append(acc_etc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.externals as extjoblib\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "filename = 'ExtraTreesClassifier1.sav'\n",
    "joblib.dump(etc, filename)\n",
    "\n",
    "loaded_model = joblib.load(filename)\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuning_Testing = {'Algorithm':Algo_tuning,'Accuracy':Testing_tuning}\n",
    "Tuning_Training = {'Algorithm':Algo_tuning,'Accuracy':Training_tuning}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuning_Testing = pd.DataFrame(Tuning_Testing)\n",
    "Tuning_Training = pd.DataFrame(Tuning_Training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.962637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.960440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.960440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.953846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.912088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Algorithm  Accuracy\n",
       "2    KNeighborsClassifier  1.000000\n",
       "4  DecisionTreeClassifier  1.000000\n",
       "7    ExtraTreesClassifier  1.000000\n",
       "5  RandomForestClassifier  0.962637\n",
       "0     Logistic Regression  0.960440\n",
       "6      AdaBoostClassifier  0.960440\n",
       "1                     SVC  0.953846\n",
       "3              GaussianNB  0.912088"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuning_Training.sort_values('Accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "      <td>0.982456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.964912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.964912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.956140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.938596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.903509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Algorithm  Accuracy\n",
       "7    ExtraTreesClassifier  0.982456\n",
       "0     Logistic Regression  0.964912\n",
       "1                     SVC  0.964912\n",
       "2    KNeighborsClassifier  0.956140\n",
       "5  RandomForestClassifier  0.947368\n",
       "6      AdaBoostClassifier  0.947368\n",
       "3              GaussianNB  0.938596\n",
       "4  DecisionTreeClassifier  0.903509"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuning_Testing.sort_values('Accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def XGBClassifier(X_train,X_test,y_train,y_test):\n",
    "    \n",
    "    # XGBoost classifier most required parameters\n",
    "    params={\n",
    "         \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "         \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "         \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "         \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "         \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] \n",
    "        }\n",
    "    # Randomized Search\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_classifier = XGBClassifier()\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    random_search = RandomizedSearchCV(xgb_classifier, param_distributions=params,cv=10, scoring= 'accuracy', n_jobs= -1, verbose= 3)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    xgb_classifier = random_search.best_estimator_\n",
    "    # Train the model using the training sets \n",
    "    xgb_classifier.fit(X_train,y_train)\n",
    "    y_pred_xgb_classifier_test = xgb_classifier.predict(X_test)\n",
    "    y_pred_xgb_classifier_train = xgb_classifier.predict(X_train)\n",
    "    # Calculating the accuracy\n",
    "    acc_xgb_classifier_test =metrics.accuracy_score(y_test, y_pred_xgb_classifier_test)\n",
    "    acc_xgb_classifier_train =metrics.accuracy_score(y_train, y_pred_xgb_classifier_train)\n",
    "\n",
    "    print( 'Accuracy of xgb_classifier model training : ', acc_xgb_classifier_train )\n",
    "    print( 'Accuracy of xgb_classifier model testing : ', acc_xgb_classifier_test )\n",
    "\n",
    "    roc_xgb_classifier_train =metrics.roc_auc_score(y_train,y_pred_xgb_classifier_train )\n",
    "    roc_xgb_classifier_test = metrics.roc_auc_score(y_test,y_pred_xgb_classifier_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[14:47:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:47:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy of xgb_classifier model training :  0.9802197802197802\n",
      "Accuracy of xgb_classifier model testing :  0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "XGBClassifier(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'XGB_model1.sav'\n",
    "# joblib.dump(xgb_classifier, filename)\n",
    "\n",
    "# loaded_model = joblib.load(filename)\n",
    "# result = loaded_model.score(X_test, y_test)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true =y_test,y_pred =y_pred_xgb_classifier_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_true =y_test,y_pred =y_pred_xgb_classifier_test).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_true =y_test,y_pred =y_pred_xgb_classifier_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Create a GradientBoostingClassifier\n",
    "GBC = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Accuracy of GBC model training :  0.9538461538461539\n",
      "Accuracy of GBC model testing :  0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameter Optimization\n",
    "parameters = [{\n",
    "\"loss\":[\"deviance\"],\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    \n",
    "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    'n_estimators': [4, 6, 9, 10, 15], \n",
    "              'max_features': ['log2', 'sqrt','auto'], \n",
    "     'max_depth': [3,5,10,15],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'min_samples_leaf': [4, 6, 8,10],\n",
    "               'min_samples_split': [5,10,15,20],\n",
    "}]\n",
    "\n",
    "# Run the grid search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "grid_obj = RandomizedSearchCV(GBC, param_distributions=parameters, scoring= 'accuracy', n_jobs= -1, verbose= 3)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the svc to the best combination of parameters\n",
    "GBC = grid_obj.best_estimator_\n",
    "\n",
    "\n",
    "# Train the model using the training sets \n",
    "GBC.fit(X_train,y_train)\n",
    "y_pred_GBC_classifier_test = GBC.predict(X_test)\n",
    "y_pred_GBC_classifier_train = GBC.predict(X_train)\n",
    "# Calculating the accuracy\n",
    "acc_GBC_classifier_test =  metrics.accuracy_score(y_test, y_pred_GBC_classifier_test) \n",
    "acc_GBC_classifier_train =  metrics.accuracy_score(y_train, y_pred_GBC_classifier_train)  \n",
    "\n",
    "print( 'Accuracy of GBC model training : ', acc_GBC_classifier_train )\n",
    "print( 'Accuracy of GBC model testing : ', acc_GBC_classifier_test )\n",
    "\n",
    "roc_GBC_classifier_train =metrics.roc_auc_score(y_train,y_pred_GBC_classifier_train )\n",
    "roc_GBC_classifier_test = metrics.roc_auc_score(y_test,y_pred_GBC_classifier_test )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
